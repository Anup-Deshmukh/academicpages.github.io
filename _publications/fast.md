---
title: "Perception of emotions from Audio Signals"
collection: ongoing
permalink: /publication/fast
date: 2018-10-05
excerpt: ''
venue: ''

---

The emotion plays a key role in many applications like in healthcare domain to gather emotional behaviors. It is very crucial to determine “how it was said” other than “what it was said”,
Therefore, in human- machine interaction applications, it is important that emotional states in human speech are fully perceived by ML model well.

Our ultimate goal here at FAST lab is to model the stress from emotions. The amount of annotated data is limited and these annotations are highly subjective. Given this common issue in affective computation, the subjectivity lies in the fact that stress perceived by one person may be completely different from the other. Since we are not classifying the data into stress or not stress labels, we must find a way to map these emotions to stress. In short, identify where stress lies in the emotion space. 

This brings us to many interesting theories in psychology like [A&V](https://en.wikipedia.org/wiki/Emotion_classification), [PAD](https://en.wikipedia.org/wiki/PAD_emotional_state_model) and [Lovheim cube](https://en.wikipedia.org/wiki/L%C3%B6vheim_cube_of_emotion). What we are trying to acheive here at FAST lab is to see if our deep model: i-CNN follows these widely celebrated theories and whether it truly can model the emotional space described by them. 

**Our results will be available soon**

